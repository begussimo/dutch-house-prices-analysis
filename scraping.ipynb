{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072d569a",
   "metadata": {},
   "source": [
    "# Web scraping for houses dataset\n",
    "In this notebook, we will create a dataset of houses found from [Funda](https://www.funda.nl/) (Dutch real-estate website). In order to do this, we need to program a web bot to retrieve all the information for us. We will use a combination of [Selenium](https://selenium-python.readthedocs.io/) and [Beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) for this.\n",
    "\n",
    "This notebook is part of my House Price series in which we create a dataset, train a prediction model, and deploy the model and an accompanying web app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284e37b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "9b225f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de5456",
   "metadata": {},
   "source": [
    "## Getting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f385a",
   "metadata": {},
   "source": [
    "First, we need to give our bot acces to the internet. For this we use Selenium. Selenium is a python package that automates web browser interaction. \n",
    "\n",
    "First we start our webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "19258d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "url=\"https://www.funda.nl/koop/heel-nederland/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6490c3",
   "metadata": {},
   "source": [
    "A screen must have popped up showing an empty browser. This is the browser that will be used by our bot. \n",
    "\n",
    "Now we do some initializations. These lists are all predefined in order for our information to be appended to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "a9dde65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adresses = [] \n",
    "cities = []\n",
    "prices = [] \n",
    "lot_sizes = []\n",
    "build_years = []\n",
    "build_types = []\n",
    "house_types = []\n",
    "roofs = []\n",
    "rooms = []\n",
    "toilets = []\n",
    "floors = []\n",
    "energylabels = []\n",
    "positions = []\n",
    "gardens = []\n",
    "\n",
    "true_features = ['Soort woonhuis', 'Soort bouw', 'Bouwjaar', 'Soort dak', 'Aantal kamers', 'Aantal badkamers', 'Aantal woonlagen', 'Energielabel', 'Ligging', 'Tuin']\n",
    "arrays = [house_types, build_types, build_years, roofs, rooms, toilets, floors, energylabels, positions, gardens]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9d3cb77",
   "metadata": {},
   "source": [
    "Now the main part starts, retrieving the data. Our bot can be told to go to a certain URL and retrieve information, but the information our bot retrieves is _ALL_ the HTML that is on the webpage. In order to make sense of this, we use Beautifulsoup. Beautifulsoup can do quick searches in HTML and XML data. \n",
    "\n",
    "In order to tell beautifulsoup which data to find, we need to explicitly mention the type of element, name of the element's class or element attributes. This causes  a lot of searching and trial and error, in order to find the correct class names and get the data out in the form that you want.\n",
    "\n",
    "Funda gives results of houses like the following, showing the adress, price, size and number of rooms:\n",
    "\n",
    "![img](assets/Funda.png)\n",
    "\n",
    "The HTML location of this information can be found, when inspecting the page. We can, for example, see that the data on the price is stored inside a <font color='red'>'data-test-id'</font> element with the class: <font color='red'>'price-sale'</font>.\n",
    "\n",
    "![img](assets/FundaPrice.png)\n",
    "\n",
    "In beautifulsoup, this can be retrieved using:\n",
    "\n",
    "```python\n",
    "price = text.find('span', attrs={'class':'search-result-price'})\n",
    "```\n",
    "\n",
    "For more information about class names and search strategies in beautifulsoup, I suggest reading an in-depth web scraping tutorial like [this one](https://www.dataquest.io/blog/web-scraping-beautifulsoup/).\n",
    "\n",
    "In our case, retrieving the data about the houses is a tad harder. This is because the search results do not give all data on the houses. More data is given on the webpage of each house listing. Our approach is therefore to navigate to this listing (retrieve URL first) and do the same process of retrieving data there too. \n",
    "\n",
    "See if you can understand this part of the code too :-)\n",
    "\n",
    "> IMPORTANT: Before running this code (or code like it that you have written yourself), make sure that you navigate to your URL in the pop up browser window first. Sometimes here you need to do a captcha or other 'prove you are not a bot' test. The bot itself cannot move past these screens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "fe4f28ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [07:46<00:00, 29.18s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://www.funda.nl/zoeken/koop/?selected_area=%5B%22nl%22%5D\" # The URL with the data you want to extract\n",
    "for j in tqdm(range(650,666)): # In this case we extract data from 500 pages of results\n",
    "    if j==1:\n",
    "        driver.get(url)\n",
    "    else:\n",
    "        driver.get(url+'&search_result='+str(j)) # Add string defining the page of the search results\n",
    "\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content) \n",
    "    for a in soup.findAll('div',href=False, attrs={'data-test-id': 'search-result-item'}): # Loop over all individual house results\n",
    "        # Find initial data\n",
    "        address = a.find('h2', attrs={'data-test-id':'street-name-house-number'})\n",
    "        city = a.find('div', attrs={'data-test-id':'postal-code-city'})\n",
    "        price = a.find('p', attrs={'data-test-id':'price-sale'})\n",
    "        lot_size = a.find('li', attrs={'class':'mr-4 flex flex-[0_0_auto]'})\n",
    "\n",
    "        # Find URL of listing and extract HTML data\n",
    "        listing_url = a.find('a', href=True, attrs={'data-test-id':'object-image-link'})\n",
    "        driver.get(listing_url['href'])\n",
    "        content_full = driver.page_source\n",
    "        soup_full = BeautifulSoup(content_full) \n",
    "        \n",
    "        # Find list of house characteristics\n",
    "        characteristics = soup_full.findAll('dl', attrs={'class':'object-kenmerken-list'})\n",
    "\n",
    "        feature_names = []\n",
    "        features = []\n",
    "\n",
    "        loop_range = np.arange(len(characteristics))\n",
    "        loop_range = loop_range[np.arange(len(loop_range))!=2]\n",
    "        loop_range = loop_range[np.arange(len(loop_range))!=4]\n",
    "        \n",
    "        # For each characteristic, retrieve the corresponding value\n",
    "        for i in loop_range:\n",
    "            for feature in characteristics[i].findAll('dt'):\n",
    "                feature_names.append(list(filter(None, feature.text.split('\\n')))[0])\n",
    "\n",
    "            for feature in characteristics[i].findAll('dd'):\n",
    "                try:\n",
    "                    features.append(list(filter(None, feature.text.split('\\n')))[0])\n",
    "                except: \n",
    "                    pass\n",
    "        \n",
    "        # Add data to corresponding predefined list        \n",
    "        for name, array in zip(true_features, arrays):\n",
    "            if name in feature_names:\n",
    "                array.append(features[feature_names.index(name)])\n",
    "            else:\n",
    "                array.append(np.nan)        \n",
    "            \n",
    "            # Append initial data to lists\n",
    "        adresses.append(address.text.split(\"\\n\")[1].lstrip())\n",
    "        prices.append(price.text.split('\\n')[1].lstrip().replace(' k.k.',\"\").replace('€ ',\"\").replace('.',\"\"))\n",
    "        cities.append(' '.join(city.text.split('\\n')[1].split(' ')[14:]))\n",
    "        lot_sizes.append(lot_size.text.split('\\n')[1].lstrip().replace(' m²',\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "48062c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vraagprijs', '    Vraagprijs per m²', 'Aangeboden sinds', 'Status', 'Aanvaarding', 'Bijdrage VvE', 'Soort appartement', 'Soort bouw', 'Bouwjaar', 'Soort dak', 'Aantal kamers', 'Aantal badkamers', 'Badkamervoorzieningen', 'Aantal woonlagen', 'Voorzieningen', 'Energielabel', 'Isolatie', 'Verwarming', 'Warm water', 'Cv-ketel', 'Ligging', 'Balkon/dakterras', 'Schuur/berging', 'Soort parkeergelegenheid', 'Inschrijving KvK', 'Jaarlijkse vergadering', 'Periodieke bijdrage', 'Reservefonds aanwezig', 'Onderhoudsplan', 'Opstalverzekering']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "6ff6263b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3360"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Address': adresses,\n",
    "                   'City': cities,\n",
    "                   'Price': prices,\n",
    "                   'Lot size (m2)': lot_sizes,\n",
    "                   'Build year': build_years,\n",
    "                   'Build type': build_types,\n",
    "                   'House type': house_types,\n",
    "                   'Roof': roofs,\n",
    "                   'Rooms': rooms,\n",
    "                   'Toilet': toilets,\n",
    "                   'Floors': floors,\n",
    "                   'Energy label': energylabels,\n",
    "                   'Position': positions,\n",
    "                   'Garden': gardens}) \n",
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87165051",
   "metadata": {},
   "source": [
    "After this has run, we can combine the lists into a DataFrame. Shown below, is a partially retrieved dataset of a 1000 entries. I retrieved data in chunks of 50-100 pages, as extracting 500 pages of data in one go took a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "4e83cac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset consists of information on 45 houses\n"
     ]
    }
   ],
   "source": [
    "print('Dataset consists of information on ' + str(len(df)) + ' houses')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055513c",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "Finally, we save the data. In this case, this was the data from page 401-500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "5ec8ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('house_data_650-666.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1d001c1",
   "metadata": {},
   "source": [
    "## Merging the files\n",
    "Here is the ETL steps to bring the different excel sheets together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "2d79040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('house_data_1-130.csv')\n",
    "df2=pd.read_csv('house_data_130-150.csv')\n",
    "df3=pd.read_csv('house_data_150-180.csv')\n",
    "df4=pd.read_csv('house_data_180-200.csv')\n",
    "df5=pd.read_csv('house_data_200-240.csv')\n",
    "df6=pd.read_csv('house_data_240-280.csv')\n",
    "df7=pd.read_csv('house_data_280-320.csv')\n",
    "df8=pd.read_csv('house_data_320-360.csv')\n",
    "df9=pd.read_csv('house_data_360-427.csv')\n",
    "df10=pd.read_csv('house_data_427-470.csv')\n",
    "df11=pd.read_csv('house_data_470-520.csv')\n",
    "df12=pd.read_csv('house_data_520-577.csv')\n",
    "df13=pd.read_csv('house_data_577-614.csv')\n",
    "df14=pd.read_csv('house_data_614-641.csv')\n",
    "df15=pd.read_csv('house_data_641-650.csv')\n",
    "df16=pd.read_csv('house_data_650-666.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "4f66f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffinal=pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "5ac1cafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9987 entries, 0 to 239\n",
      "Data columns (total 14 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Address        9987 non-null   object \n",
      " 1   City           9987 non-null   object \n",
      " 2   Price          9987 non-null   object \n",
      " 3   Lot size (m2)  9987 non-null   float64\n",
      " 4   Build year     9816 non-null   object \n",
      " 5   Build type     9974 non-null   object \n",
      " 6   House type     7940 non-null   object \n",
      " 7   Roof           9653 non-null   object \n",
      " 8   Rooms          9975 non-null   object \n",
      " 9   Toilet         9975 non-null   object \n",
      " 10  Floors         9975 non-null   object \n",
      " 11  Energy label   9974 non-null   object \n",
      " 12  Position       9443 non-null   object \n",
      " 13  Garden         8302 non-null   object \n",
      "dtypes: float64(1), object(13)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dffinal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c901595",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffi=dffinal.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a6ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffi.to_csv('house_alldata.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
